[
["index.html", "MIDFIELD Workshops Facilitators Publications Licenses Acknowledgement", " MIDFIELD Workshops The Multiple-Institution Database for Investigating Engineering Longitudinal Development (MIDFIELD) is a partnership of higher education institutions with engineering programs. MIDFIELD contains student record data from 1988–2017 for approximately one million undergraduate, degree-seeking students at the partner institutions. This site provides access to workshop materials for studying how undergraduate students maneuver through their curricula using MIDFIELD data. [For more information about MIDFIELD] Facilitators Matthew Ohland is the MIDFIELD Director and Principal Investigator. He is Professor and Associate Head of Engineering Education at Purdue University. Marisa Orr is the MIDFIELD Associate Director and Assistant Professor in Engineering and Science Education with a joint appointment in Mechanical Engineering at Clemson University. Russell Long is MIDFIELD Managing Director and Data Steward. He developed the stratified data sample for the R packages used in this workshop. Susan Lord is Director of the MIDFIELD Institute and Professor and Chair of Engineering and Professor of Electrical Engineering at the University of San Diego. Richard Layton is the MIDFIELD Director of Data Display and Professor of Mechanical Engineering at Rose-Hulman. He is the lead developer of the R packages used in this workshop. Publications The MIDFIELD team has been exploring and presenting the stories in the MIDFIELD data for several years. To see a sample of our work, you can follow these links: Lord SM, Ohland MW, Layton RA,and Camacho MM (2019) Beyond pipeline and pathways: Ecosystem metrics. Journal of Engineering Education, 108, 32–56, https://doi.org/10.1002/jee.20250 Lord SM, Layton RA, and Ohland MW (2015) Multi-Institution study of student demographics and outcomes in Electrical and Computer Engineering in the USA, IEEE Transactions on Education, 58(3), 141–150, http://dx.doi.org/10.1109/TE.2014.2344622 Brawner CE, Lord SM, Layton RA, Ohland MW, and Long RA (2015) Factors affecting women’s persistence in chemical engineering, International Journal of Engineering Education 31(6A), 1431–1447, https://tinyurl.com/y6jq58xh [For more information about MIDFIELD publications] Licenses The following licenses apply to the text, data, and code in these workshops. Our goal is to minimize legal encumbrances to the dissemination, sharing, use, and re-use of this work. However, the existing rights of authors whose work is cited (text, code, or data) are reserved to those authors. CC-BY 4.0 for all text. GPL-3 for all code. CC0 for all data. Acknowledgement Funding provided by the National Science Foundation Grant 1545667 “Expanding Access to and Participation in the Multiple-Institution Database for Investigating Engineering Longitudinal Development.” "],
["about-the-midfield-workshops.html", "1 About the MIDFIELD workshops 1.1 What is midfieldr? 1.2 Why R? 1.3 Why R graphics?", " 1 About the MIDFIELD workshops 1.1 What is midfieldr? Analytical tools for research in student pathways are generally scarce. The R package midfieldr provides an entry to this type of intersectional research. midfieldr is a R package that provides functions for turning student-record data into persistence metrics. midfielddata is an R package that provides a stratified sample of the MIDFIELD data. This data package contains student records for 98,000 students, suitable for practice with the midfieldr package. [For more information about midfieldr] [For more information about midfielddata] 1.2 Why R? R is an open source language and environment for statistical computing and graphics (R Core Team, 2018), ranked by IEEE in 2018 as the 7th most popular programming language (Python, C++, and Java are the top three) (Cass, 2018). If you are new to R, some of its best features, paraphrasing Wickham (2014a), are: R is free, open source, and available on every major platform, making it easy for others to replicate your work. More than 14,250 open-source R packages are available (14,250). Many are cutting-edge tools. R packages provide deep-seated support for data analysis, e.g., missing values, data frames, and subsetting. R packages provide powerful tools for communicating results via html, pdf, docx, or interactive websites. It is easy to get help from experts in the R community. RStudio, an integrated development environment (IDE) for R, includes a console, editor, and tools for plotting, history, debugging, and workspace management as well as access to GitHub for collaboration and version control (RStudio Team, 2016). [For more information about R] [For more information about RStudio] 1.3 Why R graphics? Charles Kostelnick (2007) writes, “The array of design options in software like Microsoft Excel and PowerPoint creates the illusion of flexibility. … So marvelously malleable are these graphical effects—but for whom and to what end? Paradoxically, then, even as the technology for visualizing data has become more sophisticated, it does not necessarily engender rhetorically sensitive design.” The graphics tools in R provide the means to control every pixel in the service of “rhetorically sensitive design.” Designers can craft their visual arguments to balance logos, ethos, pathos, and kairos as appropriate for a given audience in a given rhetorical situation. [For a gallery of R graphics] "],
["midfield-institute-2019.html", "2 MIDFIELD Institute 2019 2.1 Description 2.2 Before you arrive 2.3 Sunday agenda 2.4 Monday agenda 2.5 Tuesday agenda", " 2 MIDFIELD Institute 2019 MIDFIELD Institute June 3–4, 2019 Purdue University, West Lafayette, Indiana, USA Neil Armstrong Hall of Engineering Room B-098 (basement) Contact: Russell Long, ralong@purdue.edu with questions Registration: http://www.conf.purdue.edu/MIDFIELD. 2.1 Description We are offering the first Multiple Institution Database for Investigating Engineering Longitudinal Development (MIDFIELD) Institute on Monday June 3 and Tuesday June 4, 2019 in West Lafayette, Indiana. We welcome faculty, staff, and graduate students. Our learning objectives can be categorized in two broad classes: qualitative and computational. Qualitatively, by the end of the workshop participants should be able to: Describe the data available in MIDFIELD Describe how the MIDFIELD data are organized Describe key principles of effective data visualization Identify deficiencies of common graph types List potential resources beyond MIDFIELD that can contribute to answering research questions Computationally, participants should be able use midfieldr, an R package specifically designed for use with MIDFIELD, to: Calculate and evaluate educational metrics Produce a table of data that addresses a research question Explore and tell a story from MIDFIELD data 2.2 Before you arrive MIDFIELD Workshops for an introduction to MIDFIELD and the workshop facilitators. Getting started for pre-workshop software installation instructions—assuming you plan on using your own laptop. If not, we will have computers available onsite. Optional: For several years now, we have been using R to explore and present the stories in the MIDFIELD data. To see a sample of our data graphics, you can follow the links in the publications section. ▲ top of page 2.3 Sunday agenda 2019-06-02 This is an optional session designed for R novices. Time Activities 4:30–5:30 pm Optional. Time to help anyone needing assistance with the software installation. 5:30–6:00 pm Introductions. Introduce presenters, participants, and learning objectives. Verify that software is installed. 6:00 Pizza should arrive 6:00–6:50 R basics. The RStudio environment and R objects, functions, and scripts 7:00–7:50 Graph basics. Meet ggplot, geom layers, aesthetic mappings, and facets. Writing graphs to file. 8:00–8:50 Data basics. Data import, data structure, and data transformation. Writing data to file. 9:00 Adjourn ▲ top of page 2.4 Monday agenda 2019-06-03 Time Activities 8:00-9:00 Breakfast. Provided in the workshop room. 9:00–10:00 Introductions. Introduce presenters, participants, objectives, MIDFIELD, and persistence metrics we have found insightful. Break 10:15–12:15 Guided practice. Self-paced tutorials using midfieldr and midfielddata. 12:15–1:15 Lunch. Provided in the workshop room. 1:15–2:15 Defining your question. Examples of things to look out for and consider when exploring research questions. Break 2:30–4:30 Self-directed practice. Review the variables in midfielddata. Define a problem involving the data that interests you. Find a collaborator and begin exploring the problem. 4:30–4:45 Conclusion. Reflection and discussion Break 5:00–7:00 Reception. Purdue Union, Anniversary Drawing Room 7:00 Adjourn. Make your own dinner plans. ▲ top of page 2.5 Tuesday agenda 2019-06-04 Time Activities 8:00–9:00 Breakfast. Provided in the workshop room. 9:00–10:00 Data visualization. Deficiencies of common graphs. Creating more effective graphs. Break 10:15–10:45 Explore data. Finding and presenting stories in the data. 10:45–12:15 Self-directed work. Continue the collaborative work from Day 1. Produce data displays that address your research question. 12:15–3:15 Lunch and poster preparation. Lunch provided in the workshop room. Participants make posters to display their work-in-progress. 3:15–4:15 Poster session. 4:15–5:00 Conclusion. Summarize objectives. Plans for proposed 2020 FIE Special Session to showcase participants’ work. Assess MIDFIELD Institute. 5:00 Adjourn ▲ top of page "],
["getting-started.html", "3 Getting started with R 3.1 Install R and RStudio 3.2 Install an R package 3.3 Install midfielddata and midfieldr 3.4 Create an R project 3.5 Create directories", " 3 Getting started with R If you already have R and RStudio installed, please update to the most recent releases and update your R packages as well. If you are joining us for the first time, it is vital that you attempt to set up your computer with the necessary software in advance or it will be difficult to keep up. Unless noted otherwise, we assume the reader is an R novice. Thus the first steps are to install R and RStudio. 3.1 Install R and RStudio Windows users may have to login as an Administrator (localmgr) before installing the software. Install R for your operating system Install RStudio, a user interface for R If you need additional assistance for Mac OS or Linux, these links might be useful Install R and RStudio on Mac OS by Michael Galarnyk (or you can Google more recent instructions) How to Install R Ubuntu 16.04 Xenial by Kris Eberwein (or you can Google more recent instructions) Once the installation is complete, you can take a 2-minute tour of the RStudio interface. Please use headphones or ear-buds if you watch the video during the workshop. Let’s start (00:57–02:32) by R Ladies Sydney (Richmond, 2018) The same video includes a longer (7 minute) tour of the four quadrants (panes) in RStudio if you are interested. The RStudio quadrants (07:21–14:40) by R Ladies Sydney (Richmond, 2018) 3.2 Install an R package The fundamental unit of shareable code in R is the package. For the R novice, an R package is like an “app” for R—a collection of functions, data, and documentation for doing work in R that is easily shared with others (Wickham, 2014a). Most packages are obtained from the CRAN website (The Comprehensive R Archive Network). To install a package using RStudio: Launch RStudio The RStudio interface has several panes. We want the Files/Plots/Packages pane. Select the Packages tab Next, Click Install on the ribbon In the dialog box, type tidyverse Check the Install dependencies box Click the Install button Repeat to install the package devtools Alternatively (for future reference), if you prefer using the command-line, you can install a CRAN package (or a vector of packages) by typing install.packages() in the Console, for example, install.packages(pkgs = c(&quot;tidyverse&quot;, &quot;devtools&quot;)) Some packages are archived in a repository other than CRAN, GitHub being a current favorite. For such packages, we use install_github() from the devtools package in this form, devtools::install_github(repo = &quot;user_name/repo_name&quot;) 3.3 Install midfielddata and midfieldr In this workshop, we work with the midfieldr package and midfielddata data-package. The midfielddata package is too large to be stored in CRAN, so we use a special “drat-repository” to make the package source files available. We install these packages by typing lines of code in the Console at the prompt. The Console in the default RStudio pane layout is on the left. The R command prompt in the Console is &gt;. At the prompt, type a line of code and press Enter from your keyboard. Alternatively, you can copy a line of code from this page, paste it in the console, and press Enter. We only run these lines of code once, so you do not have to type the lines into a script. 3.3.1 Install midfielddata Install midfielddata from the our drat repo. The data package is large so this step takes time. Be patient and wait for the Console prompt &gt; to reappear. install.packages(pkgs = &quot;midfielddata&quot;, repos = &quot;https://MIDFIELDR.github.io/drat/&quot;, type = &quot;source&quot;) In the Console, load the package by typing, library(&quot;midfielddata&quot;) If successful, the Console prompt &gt; reappears. You should be able to view the midfielddata help page at this point by typing in the Console ? midfielddata which should show the help page in the RStudio Viewer pane, 3.3.2 Install midfieldr midfieldr can only be installed if the midfielddata installation was successful. devtools::install_github(repo = &quot;MIDFIELDR/midfieldr&quot;) If you receive a message like this one, see the trouble-shooting notes below. Otherwise, continue. If the midfieldr installation is successful, the Console prompt &gt; reappears. In the Console, load the package by typing, library(&quot;midfieldr&quot;) You should be able to view the midfieldr help page at this point by typing in the Console ? midfieldr which should show the help page in the RStudio Viewer pane, 3.3.3 Troubleshooting If you receive a message like the one below, then midfieldr is not installed yet. To respond to a message like this one, Press Enter to cancel Manually install the packages listed in the message using the RStudio Packages &gt; Install pane or using install.packages(\"package_name\") in the Console In this case, for example, I would type in the Console, install.packages(&quot;digest&quot;) Next Attempt to install midfieldr again: devtools::install_github(repo = \"MIDFIELDR/midfieldr\") If the message appears, manually install all packages listed one at a time Repeat until the message no longer appears and the Console responds with a prompt. Confirm the installation is successful by loading the package and checking the help page comes up. library(&quot;midfieldr&quot;) ? midfieldr 3.4 Create an R project To begin any project, we create an RStudio Project file and directory. You can recognize an R project file by its .Rproj suffix. We will create a project named after the workshop, for example, midfield_institute.Rproj, fie_workshop.Rproj, etc. If you prefer your instructions with commentary (please use headphones or ear-buds if you watch the video during the workshop), Start with a Project (02:34–04:50) by R Ladies Sydney (Richmond, 2018) If you prefer basic written instructions, RStudio, File &gt; New Project… &gt; New Directory &gt; New Project Or, click the New Project button in the Console ribbon, In the dialog box that appears, Type the workshop name as the directory name, for example, midfield_institute, fie_workshop, etc. Use the browse button to select a location on your computer to create the project folder Click the Create Project button 3.5 Create directories While file organization is a matter of personal preference, we ask that you use the directory structure shown here for your work in the workshop. Create three folders in the project main directory, where your_project is the name you gave the project, foe example, midfield_institute or fie_workshop. your_project/ ├── data/ ├── figures/ ├── scripts/ └── your_project.Rproj If you prefer your instructions with commentary (please use headphones or ear-buds if you watch the video during the workshop), Make some folders (04:50–06:08) by R Ladies Sydney (Richmond, 2018) If you prefer basic written instructions, use your usual method of creating new folders on your machine or you can use the New Folder button in the Files pane We use the folders as follows: data for data files figures for finished data displays scripts for R scripts that operate on data to produce results And that concludes the setup. "],
["day-0.html", "4 MIDFIELD Pre-Institute Workshop 4.1 R basics 4.2 Graph basics 4.3 Data basics", " 4 MIDFIELD Pre-Institute Workshop Agenda [link] This is an optional session designed for R novices. If you cannot attend this session on Sunday, you are welcome to work these tutorials on your own before the Monday/Tuesday sessions. The tutorials give the R novice a quick introduction to three essential elements of data science using R: R basics Graph basics Data basics The tutorials are designed to be completed by an R novice in less than 50 minutes each. The timing has been student-tested, but of course your mileage may vary. ▲ top of page 4.1 R basics An introduction to R adapted from (Healy, 2019a) with extra material from (Matloff, 2019). If you already have R experience, you might still want to browse this section in case you find something new. If the prerequisites have been met, the tutorial should take no longer than 50 minutes. 4.1.1 Prerequisites Every tutorial assumes that You completed the Getting started instructions You launched midfield_institute.Rproj to start the R session One of the packages is available only on GitHub. To install it, type in the Console, devtools::install_github(&quot;kjhealy/socviz&quot;) Note on usage: The double-colon notation package::name accesses a function from a specific package. In this case, for example, we are accessing the install_github() function from the devtools package. Use File &gt; New File &gt; R Script to create a new R script Name the script 01-R-basics.R Save it in the scripts directory Add a minimal header at the top of the script (if you wish) Use library() to load the packages we will use # workshop R basics # name # date library(&quot;tidyverse&quot;) library(&quot;socviz&quot;) Run the script by clicking the Source button. If you see an error like this one, Error in library(&quot;pkg_name&quot;) : there is no package called &#39;pkg_name&#39; then you should install the missing package(s) and run the script again. [To review how to install an R package] Guidelines As you work through the tutorial, type a line or chunk of code then File &gt; Save and Source. Confirm that your result matches the tutorial result. Your turn exercises give you chance to devise your own examples and check them out. You learn by doing (but you knew that already)! 4.1.2 Everything in R has a name In R, every object has a name. named entities, like x or y data you have loaded, like my_data functions you use, like sin() Some names are forbidden reserved words, like TRUE or FALSE programming words, like Inf, for, else, and function special entities, like NA and NaN Some names should not be used because they name commonly used functions q() quit c() combine or concatenate mean() range() var() variance Names in R are case-sensitive my_data and My_Data are different objects I follow the style guide used in the tidyverse by naming things in lower case, with words separated by underscores, and no spaces If you want to know if a name has already been used in a package you have loaded, go to the RStudio console, type a question mark followed by the name, e.g., ? c() ? mean() If the name is in use, a help page appears in the RStudio Help pane. 4.1.3 Everything in R is an object Origins of R objects Some objects are built in to R Some objects are loaded with packages Some objects are created by you Type this line of code in your script, Save, Source. c() is the function to combine or concatenate its elements to create a vector. c(1, 2, 3, 1, 3, 25) In these notes, everything that comes back to us in the Console as the result of running a script is shown prefaced by #&gt;. For example, after running your script, the Console should show, #&gt; [1] 1 2 3 1 3 25 But what is that [1] here? It’s just a row label. We’ll go into that later, not needed yet. We can assign the vector to a name. x &lt;- c(1, 2, 3, 1, 3, 25) y &lt;- c(5, 31, 71, 1, 3, 21, 6) To see the result in the Console, type the object name in the script, Save, and Source. (Remember, type the line of code but not the line prefaced by #&gt;—that’s the output line so you can check your results.) x #&gt; [1] 1 2 3 1 3 25 y #&gt; [1] 5 31 71 1 3 21 6 You create objects my assigning them names &lt;- is the assignment operator (keyboard shortcut: ALT –) objects exist in your R project workspace, listed in the RStudio Environment pane Datasets are also named objects, and a large number of datasets are included in the base R installation. For example,LakeHuron contains annual measurements of the lake level, in feet, from 1875–1972. LakeHuron #&gt; Time Series: #&gt; Start = 1875 #&gt; End = 1972 #&gt; Frequency = 1 #&gt; [1] 580.38 581.86 580.97 580.80 579.79 580.39 580.42 580.82 581.40 581.32 #&gt; [11] 581.44 581.68 581.17 580.53 580.01 579.91 579.14 579.16 579.55 579.67 #&gt; [21] 578.44 578.24 579.10 579.09 579.35 578.82 579.32 579.01 579.00 579.80 #&gt; [31] 579.83 579.72 579.89 580.01 579.37 578.69 578.19 578.67 579.55 578.92 #&gt; [41] 578.09 579.37 580.13 580.14 579.51 579.24 578.66 578.86 578.05 577.79 #&gt; [51] 576.75 576.75 577.82 578.64 580.58 579.48 577.38 576.90 576.94 576.24 #&gt; [61] 576.84 576.85 576.90 577.79 578.18 577.51 577.23 578.42 579.61 579.05 #&gt; [71] 579.26 579.22 579.38 579.10 577.95 578.12 579.75 580.85 580.41 579.96 #&gt; [81] 579.61 578.76 578.18 577.21 577.13 579.10 578.25 577.91 576.89 575.96 #&gt; [91] 576.80 577.68 578.38 578.52 579.74 579.31 579.89 579.96 Now you can see how the row labels work. There are 10 numbers per row, here, so the second row starts with the 11th, indicated by [11]. The last row starts with the 91st value [91] and ends with the 98th value. In the Console, type ? LakeHuron to see the help page for the data set Individual elements of a vector are obtained using [] notation. For example, the first five lake level readings are extracted with LakeHuron[1:5] #&gt; [1] 580.38 581.86 580.97 580.80 579.79 The 4th element alone, LakeHuron[4] #&gt; [1] 580.8 4.1.4 Do things in R using functions Functions do something useful functions are objects the perform actions for you functions produce output based on the input it receives functions are recognized by the parentheses at the end of their names The parentheses are where we include the inputs (arguments) to the function c() concatenates the comma-separated numbers in the parentheses to create a vector mean() computes the mean of a vector of numbers sd() computes the standard deviation of a vector of numbers summary() returns a summary of the object If we try mean() with no inputs, we get an error statement mean() #&gt; Error in mean.default() : argument &quot;x&quot; is missing, with no default If we use the Lake Huron dataset as the argument, the function is computed and displayed. Add these lines to your script, Save, and Source. mean(LakeHuron) #&gt; [1] 579.0041 sd(LakeHuron) #&gt; [1] 1.318299 summary(LakeHuron) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 576.0 578.1 579.1 579.0 579.9 581.9 We can extract subsets of data using functions. For example, If we wanted only the first five even-numbered elements, we use c() to create a vector of indices to the desired elements, LakeHuron[c(2, 4, 6, 8, 10)] #&gt; [1] 581.86 580.80 580.39 580.82 581.32 If we wanted every 5th entry over the full data set, we use length() to determine how many entries there are, and the sequence function seq() to create the vector of indices, n &lt;- length(LakeHuron) LakeHuron[seq(from = 5, to = n, by = 5)] #&gt; [1] 579.79 581.32 580.01 579.67 579.35 579.80 579.37 578.92 579.51 577.79 #&gt; [11] 580.58 576.24 578.18 579.05 577.95 579.96 577.13 575.96 579.74 Because we will be using the ggplot2 package for graphics, we will not be using the base R plot() function very often, but it is useful for a quick look at data. Add these lines to your script, Save, and Source. plot(LakeHuron) The help pages for functions are quickly accessed via the Console. In the Console type one line at a time and Enter to see the function help page. ? mean() ? sd() ? summary() 4.1.5 R functions come in packages Functions are bundled in packages Families of useful functions are bundled into packages that you can install, load, and use Packages allow you to build on the work of others You can write your own functions and packages too A lot of the work in data science consists of choosing the right functions and giving them the right arguments to get our data into the form we need for analysis or visualization Functions operate on the input you provide and give you back a result. Type the following in your script, Save, and Source. table(x) # table of counts #&gt; x #&gt; 1 2 3 25 #&gt; 2 1 2 1 sd(y) # standard deviation #&gt; [1] 25.14435 x * 5 # multiply every element by a scalar #&gt; [1] 5 10 15 5 15 125 y + 1 # add a scalar to every element #&gt; [1] 6 32 72 2 4 22 7 x + x # add elements #&gt; [1] 2 4 6 2 6 50 Comments are annotations to make the source code easier for humans to understand but are ignored by R. Comments in R are denoted by a hashtag #. 4.1.6 R objects have class Everything is an object and every object has a class. class(x) #&gt; [1] &quot;numeric&quot; class(summary) #&gt; [1] &quot;function&quot; Certain actions will change the class of an object. Suppose we try create a vector from the x object and a text string, new_vector &lt;- c(x, &quot;Apple&quot;) new_vector #&gt; [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;1&quot; &quot;3&quot; &quot;25&quot; &quot;Apple&quot; class(new_vector) #&gt; [1] &quot;character&quot; By adding the word “Apple” to the vector, R changed the class from “numeric” to “character”. All the numbers are enclosed in quotes: they are now character strings and cannot be used in calculations. The most common class of data object we will use is the data frame. titanic # data in the socviz package #&gt; fate sex n percent #&gt; 1 perished male 1364 62.0 #&gt; 2 perished female 126 5.7 #&gt; 3 survived male 367 16.7 #&gt; 4 survived female 344 15.6 class(titanic) #&gt; [1] &quot;data.frame&quot; You can see there are four variables: fate, sex, n, percent. Two variables (columns) are numeric, two are categorical. You can pick variable out of a data frame using the $ operator, titanic$percent #&gt; [1] 62.0 5.7 16.7 15.6 From the tidyverse, we will regularly use a augmented data frame called a tibble. We can convert the titanic data frame to a tibble using as_tibble(). titanic_tb &lt;- as_tibble(titanic) class(titanic_tb) #&gt; [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; titanic_tb #&gt; # A tibble: 4 x 4 #&gt; fate sex n percent #&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 perished male 1364 62 #&gt; 2 perished female 126 5.7 #&gt; 3 survived male 367 16.7 #&gt; 4 survived female 344 15.6 The tibble includes additional information about the variables 4.1.7 R objects have structure To see inside an object ask for its structure using the str() function. str(x) #&gt; num [1:6] 1 2 3 1 3 25 str(titanic) #&gt; &#39;data.frame&#39;: 4 obs. of 4 variables: #&gt; $ fate : Factor w/ 2 levels &quot;perished&quot;,&quot;survived&quot;: 1 1 2 2 #&gt; $ sex : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 2 1 2 1 #&gt; $ n : num 1364 126 367 344 #&gt; $ percent: num 62 5.7 16.7 15.6 str(titanic_tb) #&gt; Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 4 obs. of 4 variables: #&gt; $ fate : Factor w/ 2 levels &quot;perished&quot;,&quot;survived&quot;: 1 1 2 2 #&gt; $ sex : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 2 1 2 1 #&gt; $ n : num 1364 126 367 344 #&gt; $ percent: num 62 5.7 16.7 15.6 I also like to use the glimpse() function from the tidyverse. glimpse(x) #&gt; num [1:6] 1 2 3 1 3 25 glimpse(titanic) #&gt; Observations: 4 #&gt; Variables: 4 #&gt; $ fate &lt;fct&gt; perished, perished, survived, survived #&gt; $ sex &lt;fct&gt; male, female, male, female #&gt; $ n &lt;dbl&gt; 1364, 126, 367, 344 #&gt; $ percent &lt;dbl&gt; 62.0, 5.7, 16.7, 15.6 glimpse(titanic_tb) #&gt; Observations: 4 #&gt; Variables: 4 #&gt; $ fate &lt;fct&gt; perished, perished, survived, survived #&gt; $ sex &lt;fct&gt; male, female, male, female #&gt; $ n &lt;dbl&gt; 1364, 126, 367, 344 #&gt; $ percent &lt;dbl&gt; 62.0, 5.7, 16.7, 15.6 4.1.8 R does what you tell it Expect to make errors and don’t worry when that happens. You won’t break anything. Healy (2019b) offers this advice for three specific things to watch out for: Make sure parentheses are balanced—that every opening ( has a corresponding closing ). Make sure you complete your expressions. If you see a + in the Console instead of the usual prompt &gt;, that means that R thinks you haven’t written a complete expression. You can hit Esc or Ctrl C to force your way back to the Console and try correcting the code. In ggplot specifically, as you will see, we create plots layer by layer, using a + character at the end of the line—not at the beginning of the next line. For example, you would write this, ggplot(data = mpg, aes(x = displ, y = hwy)) + geom_point() not this, # error caused by incorrectly placed + ggplot(data = mpg, aes(x = displ, y = hwy)) + geom_point() To conclude, let’s make bar graph of the titanic data, ggplot(data = titanic_tb, mapping = aes(x = sex, y = n, fill = fate)) + geom_col() + coord_flip() Your turn. As shown, color distinguishes those who survived from those who did not and bar length gives totals by sex. Make a small change so that color denotes sex and bar length gives totals of survived and perished. 4.1.9 Pipe operator %&gt;% is the pipe operator from the magrittr package, part of the tidyverse suite of packages. The pipe takes the output of one statement and makes it the input of the next statement. You can think of it as the word “then”. In this example, we’ll use the starwars dataset from the dplyr package, glimpse(starwars) #&gt; Observations: 87 #&gt; Variables: 13 #&gt; $ name &lt;chr&gt; &quot;Luke Skywalker&quot;, &quot;C-3PO&quot;, &quot;R2-D2&quot;, &quot;Darth Vader&quot;, ... #&gt; $ height &lt;int&gt; 172, 167, 96, 202, 150, 178, 165, 97, 183, 182, 188... #&gt; $ mass &lt;dbl&gt; 77.0, 75.0, 32.0, 136.0, 49.0, 120.0, 75.0, 32.0, 8... #&gt; $ hair_color &lt;chr&gt; &quot;blond&quot;, NA, NA, &quot;none&quot;, &quot;brown&quot;, &quot;brown, grey&quot;, &quot;b... #&gt; $ skin_color &lt;chr&gt; &quot;fair&quot;, &quot;gold&quot;, &quot;white, blue&quot;, &quot;white&quot;, &quot;light&quot;, &quot;l... #&gt; $ eye_color &lt;chr&gt; &quot;blue&quot;, &quot;yellow&quot;, &quot;red&quot;, &quot;yellow&quot;, &quot;brown&quot;, &quot;blue&quot;,... #&gt; $ birth_year &lt;dbl&gt; 19.0, 112.0, 33.0, 41.9, 19.0, 52.0, 47.0, NA, 24.0... #&gt; $ gender &lt;chr&gt; &quot;male&quot;, NA, NA, &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;female&quot;,... #&gt; $ homeworld &lt;chr&gt; &quot;Tatooine&quot;, &quot;Tatooine&quot;, &quot;Naboo&quot;, &quot;Tatooine&quot;, &quot;Alder... #&gt; $ species &lt;chr&gt; &quot;Human&quot;, &quot;Droid&quot;, &quot;Droid&quot;, &quot;Human&quot;, &quot;Human&quot;, &quot;Human... #&gt; $ films &lt;list&gt; [&lt;&quot;Revenge of the Sith&quot;, &quot;Return of the Jedi&quot;, &quot;Th... #&gt; $ vehicles &lt;list&gt; [&lt;&quot;Snowspeeder&quot;, &quot;Imperial Speeder Bike&quot;&gt;, &lt;&gt;, &lt;&gt;,... #&gt; $ starships &lt;list&gt; [&lt;&quot;X-wing&quot;, &quot;Imperial shuttle&quot;&gt;, &lt;&gt;, &lt;&gt;, &quot;TIE Adva... The following code chunk starwars %&gt;% count(homeworld) could be read as, start with the starwars object, then count the number of observations n by homeworld Because each observation in starwars is a person, count() yields the number of people from a given homeworld. The result is: #&gt; # A tibble: 49 x 2 #&gt; homeworld n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 &lt;NA&gt; 10 #&gt; 2 Alderaan 3 #&gt; 3 Aleen Minor 1 #&gt; 4 Bespin 1 #&gt; 5 Bestine IV 1 #&gt; 6 Cato Neimoidia 1 #&gt; 7 Cerea 1 #&gt; 8 Champala 1 #&gt; 9 Chandrila 1 #&gt; 10 Concord Dawn 1 #&gt; # ... with 39 more rows The pipe makes a sequence of operations easy to construct and easy to read, starwars %&gt;% count(homeworld) %&gt;% arrange(desc(n)) %&gt;% filter(n &gt;= 3) %&gt;% drop_na() which can be read as, start with the starwars object, then count the number of observations n by homeworld, then arrange the rows in descending order of n, then filter to keep all rows where \\(n \\ge 3\\), then drop rows with NA The output is #&gt; # A tibble: 5 x 2 #&gt; homeworld n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Naboo 11 #&gt; 2 Tatooine 10 #&gt; 3 Alderaan 3 #&gt; 4 Coruscant 3 #&gt; 5 Kamino 3 4.1.10 Keyboard shortcuts In Windows, Ctrl L clears the Console Alt - creates the assignment operator &lt;- Ctrl Enter runs the selected line(s) of code in an R script Feel free to take a break before starting the next tutorial. ▲ top of page 4.2 Graph basics Decline by Randall Munroe (xkcd.com) is licensed under CC BY-NC 2.5 An introduction to ggplot2 adapted from Chapter 3 from (Healy, 2019b). If you already have R experience, you might still want to browse this section in case you find something new. If the prerequisites have been met, the tutorial should take no longer than 50 minutes. 4.2.1 Prerequisites Every tutorial assumes that You completed the Getting started instructions You launched midfield_institute.Rproj to start the R session Use File &gt; New File &gt; R Script to create a new R script Name the script 02-graph-basics.R Save it in the scripts directory Add a minimal header at the top of the script (if you wish) Use library() to load the packages we will use # workshop graph basics # name # date library(&quot;tidyverse&quot;) library(&quot;gapminder&quot;) Run the script by clicking the Source button. If you see an error like this one, Error in library(&quot;pkg_name&quot;) : there is no package called &#39;pkg_name&#39; then you should install the missing package(s) and run the script again. [To review how to install an R package] Guidelines As you work through the tutorial, type a line or chunk of code then File &gt; Save and Source. Confirm that your result matches the tutorial result. Your turn exercises give you chance to devise your own examples and check them out. You learn by doing (but you knew that already)! 4.2.2 Tidy data If the data set is “tidy”, then every row is an observation and every column is a variable. The gapminder data frame is tidy. We use glimpse() to get a look at the structure. glimpse(gapminder) #&gt; Observations: 1,704 #&gt; Variables: 6 #&gt; $ country &lt;fct&gt; Afghanistan, Afghanistan, Afghanistan, Afghanistan, ... #&gt; $ continent &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia... #&gt; $ year &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992... #&gt; $ lifeExp &lt;dbl&gt; 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.8... #&gt; $ pop &lt;int&gt; 8425333, 9240934, 10267083, 11537966, 13079460, 1488... #&gt; $ gdpPercap &lt;dbl&gt; 779.4453, 820.8530, 853.1007, 836.1971, 739.9811, 78... And we can just type its name to see the first few rows, gapminder #&gt; # A tibble: 1,704 x 6 #&gt; country continent year lifeExp pop gdpPercap #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Afghanistan Asia 1952 28.8 8425333 779. #&gt; 2 Afghanistan Asia 1957 30.3 9240934 821. #&gt; 3 Afghanistan Asia 1962 32.0 10267083 853. #&gt; 4 Afghanistan Asia 1967 34.0 11537966 836. #&gt; 5 Afghanistan Asia 1972 36.1 13079460 740. #&gt; 6 Afghanistan Asia 1977 38.4 14880372 786. #&gt; 7 Afghanistan Asia 1982 39.9 12881816 978. #&gt; 8 Afghanistan Asia 1987 40.8 13867957 852. #&gt; 9 Afghanistan Asia 1992 41.7 16317921 649. #&gt; 10 Afghanistan Asia 1997 41.8 22227415 635. #&gt; # ... with 1,694 more rows Read more about tidy data in (Wickham and Grolemund, 2017). Your turn. The ggplot2 package includes a dataset called mpg. Use glimpse() to examine the data set. How many variables? How many observations? How many of the variables are numeric? How many are character type? Is the data set tidy? Check your work. There are 234 observations and 11 variables. 4.2.3 Anatomy of a graph ggplot() is a our basic plotting function. The data = ... argument assigns the data frame. p &lt;- ggplot(data = gapminder) Next we use the mapping argument mapping = aes(...) to assign variables (column names) from the data frame to specific aesthetic properties of the graph such as the x-coordinate, the y-coordinate color, fill, etc. Here we will map the GDP per capita variable to x and the life expectancy variable to y. p &lt;- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp)) If we try to print the graph by typing the name of the graph object (everything in R is an object), we get an empty graph because we haven’t told ggplot what sort of a graph we want. p Because the graph will be a scatterplot, we add the geom_point() layer. p &lt;- p +geom_point() p # display the graph In ggplot2, “geoms” are geometric objects such as points, lines, bars, boxplots, contours, polygons, etc. You can browse the full list on the ggplot2 geom reference page. We could also have simply added the layer to the original object, p &lt;- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp)) + geom_point() p # display the graph Notice that the default axis labels are the variables names from the data frame. We can edit those with another layer p &lt;- p + labs(x = &quot;GDP per capita&quot;, y = &quot;Life expectancy (years)&quot;) p # display the graph Or, with all layers shown in one code chunk, p &lt;- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp)) + geom_point() + labs(x = &quot;GDP per capita&quot;, y = &quot;Life expectancy (years)&quot;) p # display the graph Summary. The basics steps for building up the layers of any graph consist of, assign the data frame map variables (columns names) to aesthetic properties choose geoms adjust scales, labels, etc. For more information aes() help page geom_point() help page geom_labs() help page Your turn. In the console, type ? mpg to see the data set help page. Skim the descriptions of the variables. Create a scatterplot of highway miles per gallon as a function of engine displacement in liters. Check your work: 4.2.4 Layer: smooth fit Suppose you wanted a smooth fit curve, not necessarily linear. Add a geom_smooth() layer. The name loess (pronounced like the proper name Lois) is a nonparametric curve-fitting method based on local regression. p &lt;- p + geom_smooth(method = &quot;loess&quot;, se = FALSE) p # display the graph The se argument controls whether or not the confidence interval is displayed. Setting se = TRUE yields, p &lt;- p + geom_smooth(method = &quot;loess&quot;, se = TRUE) p # display the graph For a linear-fit layer, we add a layer with method set to lm (short for linear model). The linear fit is not particularly good in this case, but now you know how to do one. p &lt;- p + geom_smooth(method = &quot;lm&quot;, se = TRUE) p # display the graph For more information geom_smooth() help page Your turn. Continue to practice with mpg. Add a loess curve fit with a confidence interval. Check your work: 4.2.5 Layer: log scale We have orders of magnitude differences in the GDP per capita variable. To confirm, we can create a summary() of the gdpPercap variable. The output shows that the minimum is 241, the median 3532, and the maximum 113523. # extract one variable from the data frame this_variable &lt;- gapminder[&quot;gdpPercap&quot;] # statistical summary of one variable summary(this_variable) #&gt; gdpPercap #&gt; Min. : 241.2 #&gt; 1st Qu.: 1202.1 #&gt; Median : 3531.8 #&gt; Mean : 7215.3 #&gt; 3rd Qu.: 9325.5 #&gt; Max. :113523.1 The bracket notation I just used, gapminder[\"gdpPercap\"], is one way to extract a variable from a data frame. In exploring a graph like this, it might be useful to add a layer that changes the horizontal scale to a log-base-10 scale. p &lt;- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp)) + geom_point() + scale_x_log10() p # display the graph The scales package allows us to change the GDP scale to dollars. Using the syntax thepackage::thefunction we can use the scales::dollar function without loading the scales package. p &lt;- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp)) + geom_point() + scale_x_log10(labels = scales::dollar) p # display the graph In this case, a linear fit might work, p &lt;- p + geom_smooth(method = &quot;lm&quot;, se = TRUE) p # display the graph Update the axis labels, p &lt;- p + labs(x = &quot;GDP per capita (log10 scale)&quot;, y = &quot;Life expectancy (years)&quot;) p # display the graph In summary, all the layers could have been be coded at once, for example, p &lt;- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = TRUE) + scale_x_log10(labels = scales::dollar) + labs(x = &quot;GDP per capita (log10 scale)&quot;, y = &quot;Life expectancy (years)&quot;) With all the layers in one place, we can see that we’ve coded all the basic steps, that is, assign the data frame map variables (columns names) to aesthetic properties choose geoms adjust scales, labels, etc. For more information scale_x_log10() help page Your turn. Continue to practice with mpg. Edit the axis labels to include units. Check your work: 4.2.6 Mapping aesthetics So far, we have mapped variables only to the x-coordinate and y-coordinate aesthetics. If we map a variable to the color aesthetic, the data symbols are automatically assigned different colors and a legend is created. In this example, we map the continent variable to color. p &lt;- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp, color = continent)) + geom_point() + scale_x_log10(labels = scales::dollar) + labs(x = &quot;GDP per capita&quot;, y = &quot;Life expectancy (years)&quot;) p # print the graph Your turn. Continue to practice with mpg. Map vehicle class to color Change the curve fit to linear Check your work: 4.2.7 Setting properties Because the colors overprint, we might try making the data symbols slightly transparent. In this case, we are not mapping a property to a variable; instead, we want all data symbols to be less opaque. The alpha argument, with \\(0 \\leq \\alpha \\leq 1\\), sets the transparency level. Because this change applies to all data points equally, we assign it in the geom, not aes(). p &lt;- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp, color = continent)) + geom_point(alpha = 0.3) + scale_x_log10(labels = scales::dollar) + labs(x = &quot;GDP per capita&quot;, y = &quot;Life expectancy (years)&quot;) p # print the graph If we add a linear fit to these data, a fit for each continent is generated. For a thinner line, I’ve added a size argument to the geom. p &lt;- p + geom_smooth(method = &quot;lm&quot;, se = FALSE, size = 0.5) p # print the graph If we want all the data markers the same color but we want to change the color, we don’t map it, we set it in the geom. Here, I’ve omitted the aesthetic mapping to color and used a color assignment in the geom. p &lt;- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp)) + geom_point(alpha = 0.3, color = &quot;purple1&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE, size = 0.5, color = &quot;purple4&quot;) + scale_x_log10(labels = scales::dollar) + labs(x = &quot;GDP per capita (log10 scale)&quot;, y = &quot;Life expectancy (years)&quot;) p # print the graph For more information R color names 4.2.8 Layer: facets In the earlier graph where we mapped continent to color, there was a lot of overprinting, making it difficult to compare the continents. The facet_wrap() layer separates the data into different panels (or facets). Like the aes() mapping, facet_wrap() is applied to a variable (column name) in the data frame. p &lt;- p + facet_wrap(facets = vars(continent)) p # print the graph Comparisons are facilitated by having the facets appear in one column, by using the ncols argument of facet_wrap(). p &lt;- p + facet_wrap(facets = vars(continent), ncol = 1) p # print the graph In a faceted display, all panels have identical scales (the default) to facilitate comparison. Again, all the layers could have been be coded at once, for example, ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp)) + geom_point(alpha = 0.3, color = &quot;purple1&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE, size = 0.5, color = &quot;purple4&quot;) + facet_wrap(facets = vars(continent), ncol = 1) + scale_x_log10(labels = scales::dollar) + labs(x = &quot;GDP per capita (log10 scale)&quot;, y = &quot;Life expectancy (years)&quot;) For more information facet_wrap() help page Your turn. Continue to practice with mpg. Map drive type to color Facet on vehicle class Add some transparency to the data symbols Omit the smooth fit Check your work: 4.2.9 Ordering the panels The default ordering of the panels in this example is alphabetical. In most cases, ordering the panels by the data (often the mean or the median) improves the display. Here we have two quantitative variables, but the one that is the more interesting is life expectancy. Our goal then is to order the continent variable by the median of the lifeExp variable in each panel. To do that, we require continent to be a factor, a type of variable specialized for creating ordered levels of a category. Using glimpse() we see that continent is already a factor (&lt;fct&gt;). glimpse(gapminder) #&gt; Observations: 1,704 #&gt; Variables: 6 #&gt; $ country &lt;fct&gt; Afghanistan, Afghanistan, Afghanistan, Afghanistan, ... #&gt; $ continent &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia... #&gt; $ year &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992... #&gt; $ lifeExp &lt;dbl&gt; 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.8... #&gt; $ pop &lt;int&gt; 8425333, 9240934, 10267083, 11537966, 13079460, 1488... #&gt; $ gdpPercap &lt;dbl&gt; 779.4453, 820.8530, 853.1007, 836.1971, 739.9811, 78... Therefore all we have to do is tell R that we want the levels of continent ordered by the median of life expectancy using the fct_reorder() function. gapminder &lt;- gapminder %&gt;% mutate(continent = fct_reorder(continent, lifeExp, median)) In doing so, I’ve overwritten the original gapminder dataset with my revised version. We set the as.table argument to false to place the panel with the highest life expectancy in the top position. ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp)) + geom_point(alpha = 0.3, color = &quot;purple1&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE, size = 0.5, color = &quot;purple4&quot;) + facet_wrap(facets = vars(continent), ncol = 1, as.table = FALSE) + scale_x_log10(labels = scales::dollar) + labs(x = &quot;GDP per capita (log10 scale)&quot;, y = &quot;Life expectancy (years)&quot;) For more information mutate() help page fct_reorder() help page Your turn. Continue to practice with mpg. Convert class to a factor ordered by the mean highway mileage Same graph as before, but order the panels by mean fuel consumption Check your work: 4.2.10 Beyond the basics Demonstrating how the basics can be built upon to create a complex data graphic. To wrap up this introduction, I’ll show you how we can use functions in various layers to show all the data in every panel; add a common overall loess smooth fit; and highlight the the continent data in each panel, making it easier to compare each continent to the global data. Because life expectancy has generally increased over time, I’m going to restrict this final graph to 2007, the most recent year in this dataset. Typing this code in your script is optional. Without further explanation, here’s the code. gapminder &lt;- gapminder %&gt;% filter(year == 2007) ggplot(data = gapminder, mapping = aes(x = gdpPercap / 1000, y = lifeExp)) + geom_point(data = select(gapminder, -continent), size = 1.25, alpha = 0.5, color = &quot;#80cdc1&quot;) + geom_smooth(data = select(gapminder, -continent), method = &quot;loess&quot;, se = FALSE, size = 0.7, color = &quot;#80cdc1&quot;) + geom_point(mapping = aes(color = continent), size = 1.25, color = &quot;#01665e&quot;) + facet_wrap(vars(continent), ncol = 1, as.table = FALSE) + labs(x = &quot;GDP per capita (thousands of dollars)&quot;, y = &quot;Life expectancy (years)&quot;, title = &quot;Life expectancy by country, 2007&quot;, caption = &quot;Source: Gapminder&quot;) + theme(legend.position = &quot;none&quot;) For more information select() help page filter() help page theme() help page ColorBrewer for color hex codes 4.2.11 Resize and write to file For consistent control over the size and aspect ratio of your publication-ready graph, you should always conclude your design by saving the image and sizing it at the same time. Here, we save the figure to the figures directory we set up earlier. ggsave(filename = &quot;figures/02-graph-basics-gapminder.png&quot;, width = 6.5, height = 10.5, units = &quot;in&quot;, dpi = 300) And the final figure looks like this: For more information ggsave() help page Your turn. Continue to practice with mpg. Write your ggsave() code chunk immediately following the ggplot() code chunk of the graph you want to save. Use ggsave to write your graph to the figures directory with the name 02-graph-basics-mpg.png Try a 6 in by 6 in figure size Check your work: Navigate to your figures folder. The new png file should be there. Open it to confirm it is the figure you expect. Feel free to take a break before starting the next tutorial. ▲ top of page 4.3 Data basics “Data preparation is not just a first step, but must be repeated many times over the course of analysis as new problems come to light or new data is collected.” (Wickham, 2014b) Indeed. In this 50-minute tutorial we introduce the basic tools of data preparation, always keeping in mind that many of the steps will be repeated many times over the course of a project. We organize our work into three main categories: Importing data Exploring data Tidying data We begin by reviewing the practice datasets available in R and functions for exploring data. 4.3.1 Prerequisites Every tutorial assumes that You completed the Getting started instructions You launched midfield_institute.Rproj to start the R session Use File &gt; New File &gt; R Script to create a new R script Name the script 03-data-basics.R Save it in the scripts directory Add a minimal header at the top of the script (if you wish) Use library() to load the packages we will use # workshop data basics # name # date library(&quot;tidyverse&quot;) library(&quot;lubridate&quot;) library(&quot;seplyr&quot;) library(&quot;rio&quot;) library(&quot;inspectdf&quot;) Run the script by clicking the Source button. If you see an error like this one, Error in library(&quot;pkg_name&quot;) : there is no package called &#39;pkg_name&#39; then you should install the missing package(s) and run the script again. [To review how to install an R package] Guidelines As you work through the tutorial, type a line or chunk of code then File &gt; Save and Source. Confirm that your result matches the tutorial result. Your turn exercises give you chance to devise your own examples and check them out. You learn by doing (but you knew that already)! 4.3.2 Data sets in R Practice data sets are included with the basic R installation and with some R packages. To list the practice data sets available in R, type in the Console, # type in the Console data() which yields #&gt; AirPassengers Monthly Airline Passenger Numbers #&gt; BJsales Sales Data with Leading Indicator #&gt; BOD Biochemical Oxygen Demand #&gt; CO2 Carbon Dioxide Uptake in Grass Plants #&gt; Formaldehyde Determination of Formaldehyde etc. We use the data() function to list practice datasets included in a package (if any). For example, to determine what packages are bundled with the dplyr package, type in the Console, # type in the Console data(package = &quot;dplyr&quot;) yields #&gt; band_instruments Band membership #&gt; band_instruments2 Band membership #&gt; band_members Band membership #&gt; nasa NASA spatio-temporal data #&gt; starwars Starwars characters #&gt; storms Storm tracks data Every data set in base R and in R packages has a help page that describes the data format and variable names. The data help page can be accessed using help(), for example, # type in the Console help(starwars, package = &quot;dplyr&quot;) Alternatively, if the package is loaded, you may run the ? item-name syntax in the Console, # type in the Console library(&quot;dplyr&quot;) ? starwars yields Your turn. These exercises assume that you have successfully followed the instructions to install midfielddata and midfieldr. Determine the names of the datasets available in the midfieldr package. Check your work Determine the variables in case_stickiness (one of the datasets in the midfieldr package). Check your work Determine the names of the datasets available in the midfielddata package. Check your work Determine the variables in midfielddegrees (one of the datasets in the midfielddata package). Check your work 4.3.3 Data structure When we first encounter any data set, the first step is to characterize its structure including, the R data structure, e.g., vector, matrix, data frame, time series, list, etc. the number of observations the number of variables the name and class of each variable To do so, we use functions like glimpse() and class() (introduced in R basics) as well as functions from the inspectdf package (introduced in the next section). [To read more about R data structures] [To read more about inspectdf] 4.3.4 Example: Create a 2018 price index Over any time period with inflation, a dollar buys less at the end the period than it did at the beginning of the period. Thus, in 1973 a single 20-year old could live comfortably on $5/hour but in 2018 (45 years later) a 20-year-old has to earn $30/hour to achieve the same modest standard of living. We usually adjust for the effects of inflation in US dollars using the Consumer Price Index (CPI) published by the US Bureau of Labor Statistics (BLS). The CPI is available by month from the BLS or from the Federal Reserve (FRED), from 1913 to the present. In this example, we obtain historical Consumer Price Index data from the Federal Reserve (FRED) and perform the necessary data carpentry to graph a price index with respect to 2018 dollars. Step 1. Importing data The FRED provides CPI data from 1913 to the present as a .txt file at the URL: http://research.stlouisfed.org/fred2/data/CPIAUCNS.txt If you click on the link, you can see that the first 13 lines are metadata. The data starts on line 14 with the column names. I’ve added line numbers to make it easier to see. 1 Title: Consumer Price Index for All Urban Consumers: All Items 2 Series ID: CPIAUCNS 3 Source: U.S. Bureau of Labor Statistics 4 Release: Consumer Price Index 5 Seasonal Adjustment: Not Seasonally Adjusted 6 Frequency: Monthly 7 Units: Index 1982-1984=100 8 Date Range: 1913-01-01 to 2019-04-01 9 Last Updated: 2019-05-10 7:42 AM CDT 10 Notes: Handbook of Methods - (https://www.bls.gov/opub/hom/pdf/cpihom.pdf) 11 Understanding the CPI: Frequently Asked Questions - 12 (https://www.bls.gov/cpi/questions-and-answers.htm) 13 14 DATE VALUE 15 1913-01-01 9.800 16 1913-02-01 9.800 etc. Assign the URL (add these lines to your script and run) url &lt;- &quot;http://research.stlouisfed.org/fred2/data/CPIAUCNS.txt&quot; Use rio::import() to download the data, skipping the first 13 lines x &lt;- rio::import(url, skip = 13) Use rio::export() to convert the txt file to CSV and write to our data directory rio::export(x, &quot;data/cpi-raw.csv&quot;) Once you have saved raw data to file (like we did above), leave that file unaltered. Never manipulate raw data manually. In this workshop, we are saving all data files to the data directory. In larger projects, one might add a data-raw directory to separate raw data files from prepared data files. We don’t have to re-import the data from the Internet every time we Source this R script, so we can comment-out the previous three lines of code with a hashtag, i.e., # url &lt;- &quot;http://research.stlouisfed.org/fred2/data/CPIAUCNS.txt&quot; # x &lt;- rio::import(url, skip = 13) # rio::export(x, &quot;data/cpi-raw.csv&quot;) Step 2. Exploring data Use rio::import() to read the raw data into the R workspace. cpi &lt;- rio::import(&quot;data/cpi-raw.csv&quot;) Use class() to determine the R data structure—in this case, cpi is a tibble (a type of data frame). class(cpi) #&gt; [1] &quot;data.frame&quot; Use glimpse() to examine the tibble—we discover the number of observations (1276), the number of variables (2), their names (DATE, VALUE), and their class or type (character, double). glimpse(cpi) #&gt; Observations: 1,276 #&gt; Variables: 2 #&gt; $ DATE &lt;chr&gt; &quot;1913-01-01&quot;, &quot;1913-02-01&quot;, &quot;1913-03-01&quot;, &quot;1913-04-01&quot;, ... #&gt; $ VALUE &lt;dbl&gt; 9.8, 9.8, 9.8, 9.8, 9.7, 9.8, 9.9, 9.9, 10.0, 10.0, 10.1... The first DATE values suggest these are monthly data, which agrees with the frequency given in the block of metadata. The VALUE variable is the CPI by month indexed at 1982–1984 (for which CPI = 100). These data are in what Hadley Wickham calls “tidy” form, that is, Each variable is in a column Each observation is a row Each value is a cell Such an organization is also called a “data matrix” or a “de-normalized form” (Mount, 2019). This form is typically the desired form for data visualization using the ggplot2 package, though other forms are sometimes useful. Functions from the inspectdf package can tell us more about the data. First, we summarize memory usage. Here, both variable use negligible memory. inspect_mem(cpi) # memory #&gt; # A tibble: 2 x 3 #&gt; col_name size pcnt #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 DATE 89.77 Kb 90.0 #&gt; 2 VALUE 10.02 Kb 10.0 Summarize the rate of missingness. Here, there are no missing values. inspect_na(cpi) # NAs #&gt; # A tibble: 2 x 3 #&gt; col_name cnt pcnt #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 DATE 0 0 #&gt; 2 VALUE 0 0 Summary 1276 observations, one per month, 1913-01 to 2019-04 1 categorical variable DATE 1 quantitative variable VALUE (the CPI) Your turn. For the midfielddegrees dataset from the midfielddata package, determine the R data structure, e.g., vector, matrix, time series, data frame, tibble, etc. the number of observations the number of variables the name and class of each variable summary of missing values summary of missing values Check your work: A tibble with 97640 observations and 5 variables. The variable that requires the most memory is id with 6.7 Mb. Three of the variables have 51% missing values. Step 3. Tidying data. For the application I have in mind, I want to convert monthly CPI to annual CPI. I start by changing the variable names (the column names) because I prefer lowercase names and I prefer meaningful file names (cpi instead of VALUE). Use dplyr::rename(new_name = old_name) to rename the variables. cpi &lt;- cpi %&gt;% dplyr::rename(date = DATE, cpi = VALUE) %&gt;% glimpse() #&gt; Observations: 1,276 #&gt; Variables: 2 #&gt; $ date &lt;chr&gt; &quot;1913-01-01&quot;, &quot;1913-02-01&quot;, &quot;1913-03-01&quot;, &quot;1913-04-01&quot;, &quot;... #&gt; $ cpi &lt;dbl&gt; 9.8, 9.8, 9.8, 9.8, 9.7, 9.8, 9.9, 9.9, 10.0, 10.0, 10.1,... Before I can average the monthly CPIs by year, I need to extract the 4-digit year from the date variable. Use lubridate::year() to extract the year from the date and mutate() to create the new year variable. cpi &lt;- cpi %&gt;% mutate(year = lubridate::year(date)) %&gt;% glimpse() #&gt; Observations: 1,276 #&gt; Variables: 3 #&gt; $ date &lt;chr&gt; &quot;1913-01-01&quot;, &quot;1913-02-01&quot;, &quot;1913-03-01&quot;, &quot;1913-04-01&quot;, &quot;... #&gt; $ cpi &lt;dbl&gt; 9.8, 9.8, 9.8, 9.8, 9.7, 9.8, 9.9, 9.9, 10.0, 10.0, 10.1,... #&gt; $ year &lt;dbl&gt; 1913, 1913, 1913, 1913, 1913, 1913, 1913, 1913, 1913, 191... Use filter() to omit 2019 data because we do not have a full year. Note the reduction in the number of observations. cpi &lt;- cpi %&gt;% filter(year != 2019) %&gt;% glimpse() #&gt; Observations: 1,272 #&gt; Variables: 3 #&gt; $ date &lt;chr&gt; &quot;1913-01-01&quot;, &quot;1913-02-01&quot;, &quot;1913-03-01&quot;, &quot;1913-04-01&quot;, &quot;... #&gt; $ cpi &lt;dbl&gt; 9.8, 9.8, 9.8, 9.8, 9.7, 9.8, 9.9, 9.9, 10.0, 10.0, 10.1,... #&gt; $ year &lt;dbl&gt; 1913, 1913, 1913, 1913, 1913, 1913, 1913, 1913, 1913, 191... Use arrange() and tail() to confirm that 2018 is the final year. cpi %&gt;% arrange(date) %&gt;% tail() #&gt; date cpi year #&gt; 1267 2018-07-01 252.006 2018 #&gt; 1268 2018-08-01 252.146 2018 #&gt; 1269 2018-09-01 252.439 2018 #&gt; 1270 2018-10-01 252.885 2018 #&gt; 1271 2018-11-01 252.038 2018 #&gt; 1272 2018-12-01 251.233 2018 Use seplyr::group_summarize() operation to determine the average annual CPI. grouping_variables &lt;- c(&quot;year&quot;) cpi_1984_basis &lt;- seplyr::group_summarize(cpi, grouping_variables, cpi = mean(cpi)) %&gt;% glimpse() #&gt; Observations: 106 #&gt; Variables: 2 #&gt; $ year &lt;dbl&gt; 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 192... #&gt; $ cpi &lt;dbl&gt; 9.883333, 10.016667, 10.108333, 10.883333, 12.825000, 15.... We have an excess of significant digits. Use round() to reduce our annual CPI values to 2 digits. cpi_1984_basis &lt;- cpi_1984_basis %&gt;% mutate(cpi = round(cpi, 2)) %&gt;% glimpse() #&gt; Observations: 106 #&gt; Variables: 2 #&gt; $ year &lt;dbl&gt; 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 192... #&gt; $ cpi &lt;dbl&gt; 9.88, 10.02, 10.11, 10.88, 12.82, 15.04, 17.33, 20.04, 17... These values can be displayed, ggplot(data = cpi_1984_basis, mapping = aes(x = year, y = cpi)) + geom_line() + labs(title = &quot;CPI with a 1983--1984 basis&quot;) As expected, in 1983–1984 (the basis years), CPI = 100. To shift the basis year to 2018, we extract the CPI for 2018, cpi_2018 &lt;- cpi_1984_basis %&gt;% filter(year == 2018) %&gt;% select(cpi) %&gt;% unlist(use.names = FALSE) cpi_2018 #&gt; [1] 251.11 Then divide all CPI values by the 2018 value to create the price index with a 2018 basis. In the basis year, the index = 1. cpi_2018_basis &lt;- cpi_1984_basis %&gt;% mutate(index = cpi / cpi_2018) %&gt;% glimpse() #&gt; Observations: 106 #&gt; Variables: 3 #&gt; $ year &lt;dbl&gt; 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 19... #&gt; $ cpi &lt;dbl&gt; 9.88, 10.02, 10.11, 10.88, 12.82, 15.04, 17.33, 20.04, 1... #&gt; $ index &lt;dbl&gt; 0.03934531, 0.03990283, 0.04026124, 0.04332763, 0.051053... Graph the price index, ggplot(data = cpi_2018_basis, mapping = aes(x = year, y = index)) + geom_line() + labs(title = &quot;Price index with a 2018 basis&quot;) The price index is used to account for inflation for any US dollar amount from 1913 to 2018 and report the results in terms of constant 2018 dollars. One simply divides the nominal dollar value by the price index for that year. Now that the data are prepared, we write them to file. rio::export(cpi_2018_basis, &quot;data/cpi.csv&quot;) 4.3.5 Example: Normalized housing costs In this example we retrieve nominal US housing costs from the OECD database. Step 1. Importing data The long URL string below retrieves US housing costs by year from the OECD database. These are nominal costs (not adjusted for inflation) and normalized to a basis year of 2015, that is, the median house price is 100 in 2015. We start by importing the data and writing it to file. url &lt;- &quot;https://stats.oecd.org/sdmx-json/data/DP_LIVE/USA.HOUSECOST.NOMINAL.IDX2015.A/OECD?contentType=csv&amp;detail=code&amp;separator=comma&amp;csv-lang=en&amp;startPeriod=1956&amp;endPeriod=2018&quot; x &lt;- rio::import(url, format = &quot;csv&quot;) rio::export(x, &quot;data/housing-raw.csv&quot;) Again, after doing this once, we can comment-out these three lines of code so we don’t have to access the internet every time we Source this script. # url &lt;- &quot;https://stats.oecd.org/sdmx-json/data/DP_LIVE/USA.HOUSECOST.NOMINAL.IDX2015.A/OECD?contentType=csv&amp;detail=code&amp;separator=comma&amp;csv-lang=en&amp;startPeriod=1956&amp;endPeriod=2018&quot; # x &lt;- rio::import(url, format = &quot;csv&quot;) # rio::export(x, &quot;data/housing-raw.csv&quot;) Step 2. Exploring data Read in the CSV file and examine it. Here, we have 49 observations of 8 variables in tidy form. housing &lt;- rio::import(&quot;data/housing-raw.csv&quot;) %&gt;% glimpse() #&gt; Observations: 49 #&gt; Variables: 8 #&gt; $ LOCATION &lt;chr&gt; &quot;USA&quot;, &quot;USA&quot;, &quot;USA&quot;, &quot;USA&quot;, &quot;USA&quot;, &quot;USA&quot;, &quot;USA&quot;, ... #&gt; $ INDICATOR &lt;chr&gt; &quot;HOUSECOST&quot;, &quot;HOUSECOST&quot;, &quot;HOUSECOST&quot;, &quot;HOUSECOST... #&gt; $ SUBJECT &lt;chr&gt; &quot;NOMINAL&quot;, &quot;NOMINAL&quot;, &quot;NOMINAL&quot;, &quot;NOMINAL&quot;, &quot;NOMI... #&gt; $ MEASURE &lt;chr&gt; &quot;IDX2015&quot;, &quot;IDX2015&quot;, &quot;IDX2015&quot;, &quot;IDX2015&quot;, &quot;IDX2... #&gt; $ FREQUENCY &lt;chr&gt; &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;,... #&gt; $ TIME &lt;int&gt; 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1... #&gt; $ Value &lt;dbl&gt; 11.16166, 12.06250, 12.92855, 14.46220, 15.78472,... #&gt; $ `Flag Codes` &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N... We asked for data starting from 1956, but the US data in this database apparently starts in 1970, hence 49 observations, one per year, from 1970 to 2019. We have two numerical variables for year and normalized housing value; of the remaining variables, one is logical and the rest are categorical. We cam summarize the categorical variables using inspect_cat(). inspect_cat(housing) #&gt; # A tibble: 6 x 5 #&gt; col_name cnt common common_pcnt levels #&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;list&gt; #&gt; 1 Flag Codes 1 &lt;NA&gt; 100 &lt;tibble [1 x 3]&gt; #&gt; 2 FREQUENCY 1 A 100 &lt;tibble [1 x 3]&gt; #&gt; 3 INDICATOR 1 HOUSECOST 100 &lt;tibble [1 x 3]&gt; #&gt; 4 LOCATION 1 USA 100 &lt;tibble [1 x 3]&gt; #&gt; 5 MEASURE 1 IDX2015 100 &lt;tibble [1 x 3]&gt; #&gt; 6 SUBJECT 1 NOMINAL 100 &lt;tibble [1 x 3]&gt; Here, the count of the number of levels for every categorical variable is 1, e.g., indicator = house cost, location = US, etc, indicating that these variables have a constant value for all years. Memory usage is small and there are no missing values. inspect_mem(housing) #&gt; # A tibble: 8 x 3 #&gt; col_name size pcnt #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 INDICATOR 504 bytes 14.7 #&gt; 2 LOCATION 496 bytes 14.5 #&gt; 3 SUBJECT 496 bytes 14.5 #&gt; 4 MEASURE 496 bytes 14.5 #&gt; 5 FREQUENCY 496 bytes 14.5 #&gt; 6 Value 440 bytes 12.9 #&gt; 7 TIME 248 bytes 7.24 #&gt; 8 Flag Codes 248 bytes 7.24 inspect_na(housing) #&gt; # A tibble: 8 x 3 #&gt; col_name cnt pcnt #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Flag Codes 49 100 #&gt; 2 LOCATION 0 0 #&gt; 3 INDICATOR 0 0 #&gt; 4 SUBJECT 0 0 #&gt; 5 MEASURE 0 0 #&gt; 6 FREQUENCY 0 0 #&gt; 7 TIME 0 0 #&gt; 8 Value 0 0 Summary 49 observations, one per year, 1970–2019 2 quantitative variables are of interest: TIME and Value the remaining categorical and logical variables are not relevant to our inquiry Step 3. Tidying data. We want to keep the TIME and Value columns and rename them. housing_2015_basis &lt;- housing %&gt;% select(TIME, Value) %&gt;% dplyr::rename(year = TIME, nominal = Value) %&gt;% glimpse() #&gt; Observations: 49 #&gt; Variables: 2 #&gt; $ year &lt;int&gt; 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, ... #&gt; $ nominal &lt;dbl&gt; 11.16166, 12.06250, 12.92855, 14.46220, 15.78472, 16.7... The costs have been normalized with a 2015 basis, that is in 2015, the price is 100. I’d like to normalize these data to 2018 costs by dividing all nominal values by the one recorded for 2018, then multiply by 100, so that the $100 price value is in the same basis year as my price index. First, find the 2018 nominal house cost. nominal_2018 &lt;- housing_2015_basis %&gt;% filter(year == 2018) %&gt;% select(nominal) %&gt;% unlist(use.names = FALSE) nominal_2018 #&gt; [1] 120.3783 Then adjust the costs so that the basis year is 2018. housing_2018_basis &lt;- housing_2015_basis %&gt;% mutate(nominal = nominal / nominal_2018 * 100) Check the tail of the data frame to confirm that the 2018 value is 100. tail(housing_2018_basis, n = 10L) #&gt; year nominal #&gt; 40 2009 73.30078 #&gt; 41 2010 71.12164 #&gt; 42 2011 68.12677 #&gt; 43 2012 70.10484 #&gt; 44 2013 75.07875 #&gt; 45 2014 78.88267 #&gt; 46 2015 83.07147 #&gt; 47 2016 88.01283 #&gt; 48 2017 93.81684 #&gt; 49 2018 100.00000 We can graph this for a look at relative nominal housing costs in the US. ggplot(data = housing_2018_basis, mapping = aes(x = year, y = nominal)) + geom_line() + labs(title = &quot;Normalized US median housing cost&quot;, subtitle = &quot;Not adjusted for inflation&quot;) Now that the data are prepared, we write them to file. rio::export(housing_2018_basis, &quot;data/housing.csv&quot;) 4.3.6 Example: Real US housing costs In this final example, we join the two data frames we created previously and adjust the US housing costs for inflation. The raw data have already been prepared, so we can continue step 3. Step 3. Tidying data. Read in the two data frames we saved earlier. Recall that both have been transformed to use a 2018 basis. cpi &lt;- rio::import(&quot;data/cpi.csv&quot;) housing &lt;- rio::import(&quot;data/housing.csv&quot;) The CPI data spans 1913 to 2018, with the index = 1 in 2018. head(cpi) #&gt; year cpi index #&gt; 1 1913 9.88 0.03934531 #&gt; 2 1914 10.02 0.03990283 #&gt; 3 1915 10.11 0.04026124 #&gt; 4 1916 10.88 0.04332763 #&gt; 5 1917 12.82 0.05105332 #&gt; 6 1918 15.04 0.05989407 tail(cpi) #&gt; year cpi index #&gt; 101 2013 232.96 0.9277209 #&gt; 102 2014 236.74 0.9427741 #&gt; 103 2015 237.02 0.9438891 #&gt; 104 2016 240.01 0.9557963 #&gt; 105 2017 245.12 0.9761459 #&gt; 106 2018 251.11 1.0000000 The housing data spans 1970 to 2018, with nominal cost = 100 in 2018. head(housing) #&gt; year nominal #&gt; 1 1970 9.272153 #&gt; 2 1971 10.020497 #&gt; 3 1972 10.739936 #&gt; 4 1973 12.013966 #&gt; 5 1974 13.112603 #&gt; 6 1975 13.930902 tail(housing) #&gt; year nominal #&gt; 44 2013 75.07875 #&gt; 45 2014 78.88267 #&gt; 46 2015 83.07147 #&gt; 47 2016 88.01283 #&gt; 48 2017 93.81684 #&gt; 49 2018 100.00000 Use left_join() to combine the two data frames, joining cpi to housing housing_cpi &lt;- left_join(housing, cpi, by = &quot;year&quot;) head(housing_cpi) #&gt; year nominal cpi index #&gt; 1 1970 9.272153 38.83 0.1546334 #&gt; 2 1971 10.020497 40.49 0.1612441 #&gt; 3 1972 10.739936 41.82 0.1665406 #&gt; 4 1973 12.013966 44.40 0.1768149 #&gt; 5 1974 13.112603 49.31 0.1963681 #&gt; 6 1975 13.930902 53.82 0.2143284 tail(housing_cpi) #&gt; year nominal cpi index #&gt; 44 2013 75.07875 232.96 0.9277209 #&gt; 45 2014 78.88267 236.74 0.9427741 #&gt; 46 2015 83.07147 237.02 0.9438891 #&gt; 47 2016 88.01283 240.01 0.9557963 #&gt; 48 2017 93.81684 245.12 0.9761459 #&gt; 49 2018 100.00000 251.11 1.0000000 Usage: left_join(x, y, by) returns all rows from x and all columns from x and y. ‘by’ is a character vector of variables (column names) that x and y have in common used to join the two data frames. In this case, we left-join cpi (y) to housing (x) because housing has fewer years. Had we reversed the order of the two data frames, rows in x with no match in y will have NA values in the new columns, i.e., test &lt;- left_join(cpi, housing, by = &quot;year&quot;) head(test) #&gt; year cpi index nominal #&gt; 1 1913 9.88 0.03934531 NA #&gt; 2 1914 10.02 0.03990283 NA #&gt; 3 1915 10.11 0.04026124 NA #&gt; 4 1916 10.88 0.04332763 NA #&gt; 5 1917 12.82 0.05105332 NA #&gt; 6 1918 15.04 0.05989407 NA tail(test) #&gt; year cpi index nominal #&gt; 101 2013 232.96 0.9277209 75.07875 #&gt; 102 2014 236.74 0.9427741 78.88267 #&gt; 103 2015 237.02 0.9438891 83.07147 #&gt; 104 2016 240.01 0.9557963 88.01283 #&gt; 105 2017 245.12 0.9761459 93.81684 #&gt; 106 2018 251.11 1.0000000 100.00000 The real housing cost is determined by dividing the nominal cost by the price index, housing_cpi &lt;- housing_cpi %&gt;% mutate(real = nominal / index) head(housing_cpi) #&gt; year nominal cpi index real #&gt; 1 1970 9.272153 38.83 0.1546334 59.96215 #&gt; 2 1971 10.020497 40.49 0.1612441 62.14490 #&gt; 3 1972 10.739936 41.82 0.1665406 64.48841 #&gt; 4 1973 12.013966 44.40 0.1768149 67.94656 #&gt; 5 1974 13.112603 49.31 0.1963681 66.77562 #&gt; 6 1975 13.930902 53.82 0.2143284 64.99793 tail(housing_cpi) #&gt; year nominal cpi index real #&gt; 44 2013 75.07875 232.96 0.9277209 80.92816 #&gt; 45 2014 78.88267 236.74 0.9427741 83.67080 #&gt; 46 2015 83.07147 237.02 0.9438891 88.00978 #&gt; 47 2016 88.01283 240.01 0.9557963 92.08325 #&gt; 48 2017 93.81684 245.12 0.9761459 96.10944 #&gt; 49 2018 100.00000 251.11 1.0000000 100.00000 Graphing normalized median US housing costs in real (constant 2018) dollars, we have, ggplot(data = housing_cpi, mapping = aes(x = year, y = real)) + geom_line() + labs(title = &quot;Normalized US median housing cost&quot;, subtitle = &quot;Constant 2018 dollars&quot;) From the graph, a monthly housing cost of $100 in 2018 is equivalent to approximately $70 in 1995 and $60 in 1970, for the same level of housing in constant 2018 dollars. Inflation has been accounted for, so the rises and falls we see in the graph represents changes in the real cost of housing. The ratios hold for any amount, so a monthly housing cost of $1000 in 2018 is equivalent to approximately $700 in 1995 and $600 in 1970—again, in constant 2018 dollars. In summary, we have used the OECD housing index to account for the fluctuations in house prices and we have used the CPI to account for inflation. 4.3.7 Summary of functions introduced Summary of the data preparation functions used in this tutorial, organized by package. dplyr arrange() order the rows of data frame filter() choose rows of a data frame that meet a condition, and omit all others mutate() add new variables (columns) to a data frame select() choose variables (columns) of a data frame and omit all others rename() rename selected variables (columns) of a data frame and keep all others left_join() combine two data frames by variables they have in common rio import() read a data frame by URL or filename export() write a data frame as a specified type of file inspectdf inspect_mem() summarize memory usage of a data frame inspect_na() summarize the rate of missingness of variables in a data frame inspect_ cat() summarize the levels within each categorical variable in a data frame lubridate year() return the year as a number from a date object seplyr group_summarize() perform summary operations on groups defined by selected variables base R head() returns the first part of an R object tail() returns the last part of an R object mean() compute an arithmetic mean round() round to a specified number of decimal places unlist() simplify a list to a vector of values ▲ top of page "],
["day-1.html", "5 MIDFIELD Institute Day 1 Prerequisites", " 5 MIDFIELD Institute Day 1 Agenda [link] Prerequisites We assume you have completed all of the Getting started instructions. Run midfield_institute.Rproj to start every work session "],
["day-2.html", "6 MIDFIELD Institute Day 2 Prerequisites", " 6 MIDFIELD Institute Day 2 Agenda [link] Prerequisites We assume you have completed all of the Getting started instructions. Run midfield_institute.Rproj to start every work session "],
["references.html", "References", " References Cass S (2018) The 2018 top programming languages. IEEE Spectrum https://tinyurl.com/ya9nbejt Healy K (2019a) Get Started. Data Visualization: A Practical Introduction. Princeton University Press, Princeton, NJ, 32–53 Healy K (2019b) Data Visualization: A Practical Introduction. Princeton University Press, Princeton, NJ https://kieranhealy.org/publications/dataviz/ Kostelnick C (2007) The visual rhetoric of data displays: The conundrum of clarity. IEEE Transactions on Professional Communication 50(2), 280–294 Matloff N (2019) fasteR: Fast Lane to Learning R! https://github.com/matloff/fasteR Mount J (2019) What is \"Tidy Data\"? Win Vector LLC http://www.win-vector.com/blog/2019/05/what-is-tidy-data/ R Core Team (2018) R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria https://www.R-project.org/ Richmond J (2018) Basic Basics Lesson 1: An opinionated tour of RStudio. https://youtu.be/kfcX5DEMAp4 RStudio Team (2016) RStudio: Integrated Development Environment for R. RStudio, Inc., Boston, MA http://www.rstudio.com/ The Comprehensive R Archive Network https://cran.r-project.org/ Wickham H (2014a) Advanced R. Taylor &amp; Francis Wickham H (2014b) Tidy Data. Journal of Statistical Software 59(10), 1–23 (doi:10.18637/jss.v059.i10) Wickham H and Grolemund G (2017) R for Data Science. O’Reilly Media, Inc., Sebastopol, CA https://r4ds.had.co.nz/ "]
]
